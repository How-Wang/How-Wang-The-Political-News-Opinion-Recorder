# -*- coding: utf-8 -*-
"""v0_combine_opinions_and_cluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A4QCfXyd2HIyNnSg4W0gbr7RIll9iLAM

# V0 意見提取 完整流程

1. 代名詞代換
2. 新聞分群
3. 找出結尾與開頭
4. 使用 K-means 分群、cosine-similarity 判斷相似性
5. 開始實作
"""


"""## 1.代名詞代換

### function 定義

#### 將一字名稱代換
- 呼叫 change_ner_one_word_name( ) 即可  
- ex: `鄭(麗文)` 痛罵 `蘇(貞昌)`
"""

def get_person_entity_list(ner_list):
  # Get the list contain changed person candidates
  person_entity_list = []
  for ner_item in ner_list:
    if ((ner_item[1] in ["PERSON"]) and (2<=len(ner_item[0])<=4) ) or ((ner_item[1] in ["ORG","NORP"]) and (3 <= len(ner_item[0]))): # 
      already_inside = False
      # run through all person in candidates
      for person_item in person_entity_list:
        # have exist in candidate, count add 1
        if ner_item[0] == person_item[0]:
          already_inside = True
          person_item[1] += 1
      # not exist in candidate, append it
      if already_inside == False:
        person_entity_list.append([ner_item[0],0])
  return person_entity_list

def remove_same_first_name_person(person_entity_list):
  # calculate the show count, in order to change person candidate place having same first name
  for item1 in person_entity_list:
    for item2 in person_entity_list:
      # if item not the same, but have same first name
      # move the less show up one to last
      if item1 != item2 and item1[0][0] == item2[0][0]:
        # move less one to last
        if item1[1] < item2[1]:
          person_entity_list.append(person_entity_list.pop(person_entity_list.index(item1)))
        elif item1[1] > item2[1]:
          person_entity_list.append(person_entity_list.pop(person_entity_list.index(item2)))
        elif len(item1[0]) > len(item2[0]):
          person_entity_list.append(person_entity_list.pop(person_entity_list.index(item1)))
        else:
          person_entity_list.append(person_entity_list.pop(person_entity_list.index(item2)))
  return person_entity_list

def change_ner_one_word_name(input_document):
  input_document = input_document.replace(" ","")
  doc = CkipDocument(raw=input_document)
  # Named-Entity Recognition
  pipeline.get_ner(doc)
  ner_list = doc.ner.to_list()
  # change 2d to 1d array
  ner_list = list(chain.from_iterable(ner_list))
  # temp save original ner for later to return
  original_ner_list = [ner[0] for ner in ner_list.copy()]
  
  # call "person entity function"
  person_entity_list = get_person_entity_list(ner_list)
  # call "remove same first name person" function
  person_entity_list = remove_same_first_name_person(person_entity_list)
  # leave person name
  person_entity_list = list([item[0] for item in person_entity_list])

  # sort the place of ner list
  ner_list.sort(key=lambda x: x[2][0], reverse=False)
  # leave the PERSON ner
  ner_list = [[ner_item[0],ner_item[1],(ner_item[2][0],ner_item[2][1])] for ner_item in ner_list if ner_item[1]=='PERSON']

  # change one word name to person entity name
  for i in range(len(ner_list)):
    if ner_list[i][0] not in person_entity_list and ner_list[i][1]=='PERSON' and len(ner_list[i][0]) < 3:
      for person_entity_item in person_entity_list:
        # first name is same, need change
        if ner_list[i][0][0] == person_entity_item[0]:
          new_word_len = len(person_entity_item)
          old_word_len = len(ner_list[i][0])
          input_document = input_document[:ner_list[i][2][0]] + person_entity_item + input_document[ner_list[i][2][1]:]
          for j in range(i+1,len(ner_list)):
            ner_list[j][2] = (ner_list[j][2][0] + (new_word_len-old_word_len), ner_list[j][2][1] + (new_word_len-old_word_len))
  return person_entity_list,input_document,original_ner_list

"""## 2. 新聞分群"""

def get_news_cluster(article_list):
    topics,_ = topic_model.fit_transform(article_list)
    representative_docs_dict = topic_model.get_representative_docs()
    return topics, representative_docs_dict

"""## 3. 利用 rule-based 找出結尾與開頭
- 根據對應到的自定義動詞
- 回頭找被對應到的人名
- 動主詞距離不超過 30 字
- 意見大於 10 字
- 主詞前不可有介係詞

### function 定義
"""

def get_ner_contain_place_list(article,people_name_list):
  # after change one name, 
  # find all name place
  ner_contain_place_list = []
  for name in people_name_list:
    # print(name)
    try:
      for m in re.finditer(name, article):
        # print(m)
        temp_ner_item = (m.start(0), m.end(0), name)
        ner_contain_place_list.append(temp_ner_item)
    except:
      continue
  # sort list
  ner_contain_place_list.sort(key=lambda x:x[0])

  return ner_contain_place_list

"""the result i want~
[{'start': 32, 'end': 45, 'text': '兼任民進黨主席的總統蔡英文', 'labels': ['person']}, {'start': 143, 'end': 260, 'text': '蔡英文在中常會上表示,國慶日剛過,今年她針對台灣當前的內外挑戰,提出了「四個堅持」的主張,也希望朝野政黨,能夠依循這2300萬台灣人民的最大公約數,作為我們對內運行民主政治、對外團結一致的共同立場,一起來因應國家內外的各項艱鉅挑戰。 ', 'labels': ['opinion']}]
"""

'''upper format is what i want!!!'''

def get_rule_based_opinion(article,ner_contain_place_list):
    # print(type(ner_contain_place_list))
    people_opinion_list =[]
    # find the verb and opinion end index by regux
    pattern = "(提到|說明|說|表示|指出|說道|宣布|解釋|強調|有感而發|表示|回應|強調|指出|解釋|批評|評估|提出|呼籲|質疑|認為|不禁問道|發現|痛批)(，|:「|：|:|;|,)*(.+?)(。)(」)*"
    result = [(m.start(3), m.end(0), m.start(1),m.end(1)) for m in re.finditer(pattern, article)]
    # print(result)
    order = 0
    for (begin, end, verb_begin, verb_end) in result:
        # print(begin, end, verb)
        # if begin begin end is too close, next one
        if end - begin < 10:
            continue
        # got spken words
        spoken_words = article[begin:end]
        # got verb words
        verb_words = article[verb_begin:verb_end]
        # find name ner candidate (in front of verb)
        filtered_ner_list = [item for item in ner_contain_place_list if (item[1]<=begin) ]
        # set words i don't want before person name
        not_want_words_list = ['於','在','受']
        # default set the person i want, as close as possible to verb
        words_index = -1
        # start to find opinions person
        while(1):
            if -words_index > len(filtered_ner_list):
                break
            try:
                if any(not_want_words in article[filtered_ner_list[words_index][0]-3:filtered_ner_list[words_index][0]] for not_want_words in not_want_words_list):
                    # if before ner, there is the word i don't want. 
                    # discard and change to other candidate
                    words_index -= 1
                else:
                    # got the person!
                    person_name = filtered_ner_list[words_index][2]
                    if begin - filtered_ner_list[words_index][0] <= 30:
                        # if not too long, accept it
                        people_opinion_list.append([[{'start':filtered_ner_list[words_index][0], 
                                                    'end':filtered_ner_list[words_index][1], 
                                                    'text':person_name,
                                                    'labels': ['person']
                                                    },{
                                                    'start': begin,
                                                    'end': end, 
                                                    'text': spoken_words,
                                                    'labels': ['opinion']
                                                    },{
                                                    'start': verb_begin,
                                                    'end': verb_end, 
                                                    'text': verb_words,
                                                    'labels': ['verb']
                                                    }
                                                    ],order])
                        order += 1
                    break
            except:
                break
            
    return people_opinion_list

"""## 4. 使用 K-means 並串接 Dataset
1. 使用 sentenceTransformer 套件作為 embedder
2. 找出最適合的分群群數
3. 找出每一群中，最接近中心點的句子作為代表
4. 根據 model clustered label list 分配每個句子到所屬的 clustered list 
5. 印出最終結果

參考資料  
https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/kmeans.py  
https://www.sbert.net/docs/pretrained_models.html  
https://stackoverflow.com/questions/19197715/scikit-learn-k-means-elbow-criterion    
https://stackoverflow.com/questions/21660937/get-nearest-point-to-centroid-scikit-learn  
"""

def cluster_article_opinions(combine_opinions_dict):
    opinions_after_cluster  = []
    for name_key in combine_opinions_dict.keys():
        order_list = [item[2] for item in combine_opinions_dict[name_key]]
        verb_list = [item[0] for item in combine_opinions_dict[name_key]]
        corpus = [item[1] for item in combine_opinions_dict[name_key]]
        corpus_embeddings = embedder.encode(corpus)

        if len(corpus)== 1:
            # print('no cluster :')
            # print("person: ",name_key)
            # print(corpus[0])
            opinions_after_cluster.append({"person":name_key,"verb":verb_list[0],"opinion":corpus[0],"order":order_list[0]})
            continue
        elif len(corpus)== 2:
            # compare the cosine similarity if bigger than 0.8, choose shorter one
            cosine_scores = util.cos_sim(corpus_embeddings[0], corpus_embeddings[1])
            # print(corpus)
            # print(cosine_scores)
            if cosine_scores > 0.8:
                if(len(corpus_embeddings[0]) < len(corpus_embeddings[1])):
                    opinions_after_cluster.append({"person":name_key,"verb":verb_list[0],"opinion":corpus[0],"order":order_list[0]})
                else:
                    opinions_after_cluster.append({"person":name_key,"verb":verb_list[1],"opinion":corpus[1],"order":order_list[1]})
            continue
        
        # Find out the best score
        sil_coeff_list = []
        least_cluster_num = 2
        if  5 >= len(corpus) > 3:
            least_cluster_num = len(corpus) - 2
        elif  7 >= len(corpus_embeddings) > 5:
            least_cluster_num = len(corpus_embeddings) - 3
        elif len(corpus_embeddings) > 7:
            least_cluster_num = len(corpus_embeddings) - 4

        for n_cluster in range(least_cluster_num, len(corpus_embeddings)):
            clustering_model = KMeans(n_clusters=n_cluster).fit(corpus_embeddings)
            label = clustering_model.labels_
            sil_coeff = silhouette_score(corpus_embeddings, label, metric='euclidean')
            sil_coeff_list.append(sil_coeff)

        # print('least_cluster_num',least_cluster_num)
        # print('len corpus',len(corpus))
        # print('max index in coeff list',sil_coeff_list.index(max(sil_coeff_list)))


        num_clusters = sil_coeff_list.index(max(sil_coeff_list)) + least_cluster_num
        plt.plot(range(least_cluster_num,len(corpus_embeddings)), sil_coeff_list)
        # print("best clusters num: ",num_clusters)

        # Perform k-means clustering
        clustering_model = KMeans(n_clusters=num_clusters)
        clustering_model.fit(corpus_embeddings)
        cluster_label_list = clustering_model.labels_
        # print(cluster_label_list)

        # Find the closet sentence in every cluster to represent group meaning
        closest_index_list, _ = pairwise_distances_argmin_min(clustering_model.cluster_centers_, corpus_embeddings)
        # print("closest_index_list: ",closest_index_list)

        # Group all sentences into their clustered_sentences_list
        clustered_sentences_list = [[] for i in range(num_clusters)]
        for sentence_id, cluster_id in enumerate(cluster_label_list):
            clustered_sentences_list[cluster_id].append(corpus[sentence_id])

        # print out result
        # print("person: ", name_key)
        for i, clustered_sentences in enumerate(clustered_sentences_list):
            # print("Cluster ", i+1)
            # print("all sentence: ",clustered_sentences)
            # print("represent sentence:",corpus[closest_index_list[i]])
            # print("")
            opinions_after_cluster.append({"person":name_key, "verb":verb_list[closest_index_list[i]], "opinion":corpus[closest_index_list[i]], "order":order_list[closest_index_list[i]]})
    
    return opinions_after_cluster

def remove_duplicate_same_article_sentence(article_opinions):
    sentences = [opinion_object['opinion'] for opinion_object in article_opinions]
    paraphrases = util.paraphrase_mining(embedder, sentences)
    for paraphrase in paraphrases:
        score, i, j = paraphrase
        if score > 0.95:
            # print("{} \n {} \n Score: {:.4f}\n\n".format(sentences[i], sentences[j], score))
            if len(sentences[i]) <= len(sentences[j]):
                article_opinions[i] = ""
            else:
                article_opinions[j] = ""
    article_opinions = [x for x in article_opinions if x]

    return article_opinions

def top(file_full_name):
    """## 載檔"""
    news_df = pd.read_csv(file_full_name)

    """## 5. 開始實作

    ### 1.
    """
    input_document_list = news_df['article'].tolist()
    document_coref_list = []
    people_name_list = []
    article_ner_list = []
    for i in trange(len(input_document_list)):
        try:
            article_person_name,document_coref,article_ner = change_ner_one_word_name(input_document_list[i])
            people_name_list.append(article_person_name)
            document_coref_list.append(document_coref)
            article_ner_list.append(article_ner)
        # print(document_coref)
        except:
            people_name_list.append("")
            document_coref_list.append("")
            article_ner_list.append("")
    # if i == stop_point:
    #   break

    article_coref_df = pd.Series(document_coref_list)
    news_df['article_coref'] = article_coref_df

    people_name_df = pd.Series(people_name_list)
    news_df['people_name'] = people_name_df

    article_ner_df = pd.Series(article_ner_list)
    news_df['ner'] = article_ner_df


    """### 2."""

    topics, representative_docs_dict = get_news_cluster(news_df['article'])


    news_df['topic'] = topics
    news_df['representative_docs'] = ""
    news_df['representative_docs'].values[:] = 0

    # print(representative_docs_dict)

    for topics_index in range(len(representative_docs_dict)):
        for representative_article in representative_docs_dict[topics_index]:
            news_df.loc[news_df['article'] == representative_article ,'representative_docs'] = 1

    # topic_info_df = pd.DataFrame()
    topic_index = []
    topic_article_index = []
    topic_represent_article = []
    topic_ner_dic = []
    for i in range(max(topics)+1):
        topic_index.append(i)
        topic_article_index.append([])
        topic_represent_article.append([])
        topic_ner_dic.append({})
    # print(topic_ner_dic)
    # print(max(topics))
    # print(len(topic_ner_dic))

    for i in range(len(news_df)):
        # print(type(news_df.iloc[i]['ner']))
        try:
            news_ner_list = news_df.iloc[i]['ner']
        except:
            news_ner_list = ast.literal_eval(news_df.iloc[i]['ner'])
        for ner_name in news_ner_list:
            if ner_name in topic_ner_dic[news_df.iloc[i]['topic']].keys():
                topic_ner_dic[news_df.iloc[i]['topic']][ner_name] += 1
            else:
                topic_ner_dic[news_df.iloc[i]['topic']][ner_name] = 0
        topic_article_index[news_df.iloc[i]['topic']].append(i)
        if news_df.iloc[i]['representative_docs']== 1:
            topic_represent_article[news_df.iloc[i]['topic']].append(i)

    topic_info_df = pd.DataFrame(list(zip(topic_index,topic_represent_article,topic_ner_dic)),columns=['topic_index','topic_represent_article','topic_ner_dic'])


    """### 3."""

    news_df['opinions_list'] = ""

    for i in trange(len(news_df)): # 1
        try:
            ner_contain_place_list = get_ner_contain_place_list(news_df.iloc[i]['article_coref'],  news_df.iloc[i]['people_name']) #ast.literal_eval()
            news_df['opinions_list'][i] = get_rule_based_opinion(news_df.iloc[i]['article_coref'], ner_contain_place_list)
        except:
            news_df['opinions_list'][i] = []

    news_df.tail(1)


    """### 4."""

    all_opinions_after_cluster = []
    for i in trange(len(news_df)):
        # print('-'*200,'\narticle: ',i)
        try:
            name_list = ast.literal_eval(news_df.iloc[i]['people_name'])
        except:
            name_list = []
        # print(name_list)
        # print('name list',news_df.iloc[i]['people_name'])
        if type(news_df.iloc[i]["opinions_list"]) == str:
            opinions_list = ast.literal_eval(news_df.iloc[i]["opinions_list"])
        else:
            opinions_list = news_df.iloc[i]["opinions_list"]
        combine_opinions_dict = {}
        for opinions in opinions_list:
            order = opinions[1]
            for opinion_item_dict in opinions[0]:
                if opinion_item_dict['labels'][0] == 'person':
                    person_name = opinion_item_dict['text']
                elif opinion_item_dict['labels'][0] == 'verb':
                    verb_words = opinion_item_dict['text']
                elif opinion_item_dict['labels'][0] == 'opinion':
                    opinion_words = opinion_item_dict['text']
            # the opinions in reasonable count
            if 10 < len(opinion_words) < 150: #  and opinions['person'] in name_list
                if person_name in combine_opinions_dict.keys():
                    combine_opinions_dict[person_name].append([verb_words,opinion_words,order])
                else:
                    combine_opinions_dict[person_name] = []
                    combine_opinions_dict[person_name].append([verb_words,opinion_words,order])
        # print(combine_opinions_dict)
        opinions_after_cluster = cluster_article_opinions(combine_opinions_dict)
        # print(i, opinions_after_cluster)
        # print()
        all_opinions_after_cluster.append(opinions_after_cluster)
        # print()
            # print(opinions['person'], opinions['verb'],":", opinions['opinion'])
    print(len(all_opinions_after_cluster))

    for i in trange(len(all_opinions_after_cluster)):
        try:
            all_opinions_after_cluster[i] = remove_duplicate_same_article_sentence(all_opinions_after_cluster[i])
        except:
            pass

    # for i,opinions_list in enumerate(all_opinions_after_cluster):
    #     print('-'*100)
    #     print(i)
    #     for opinions in opinions_list:
    #         print(opinions['person'], opinions['verb'],":", opinions['opinion'])

    s = pd.Series(all_opinions_after_cluster,name='all_opinions_after_cluster')
    new_cluster_opinions_df = pd.concat([news_df['title'],news_df['article'], s], axis=1)

    return news_df,new_cluster_opinions_df,topic_info_df

if __name__ == '__main__':
    from ckipnlp.pipeline import CkipPipeline, CkipDocument, CkipCorefPipeline
    import regex as re
    import pandas as pd
    from itertools import chain
    from tqdm import trange
    import ast
    from sentence_transformers import SentenceTransformer, util
    from sklearn.cluster import KMeans
    from sklearn.metrics import pairwise_distances_argmin_min, silhouette_score
    import matplotlib.pyplot as plt
    from bertopic import BERTopic
    import warnings
    warnings.filterwarnings("ignore")
    import argparse
    import pathlib

    """#### 輸入 ckip 帳號密碼"""

    pipeline = CkipPipeline(opts={'con_parser': {'username': 'jameshuang890902', 'password': '890902'}})
    coref_pipeline = CkipCorefPipeline(opts={'con_parser': {'username': 'jameshuang890902', 'password': '890902'}})
    topic_model = BERTopic(embedding_model="ckiplab/bert-base-chinese")
    embedder = SentenceTransformer('ckiplab/bert-base-chinese')

    parser = argparse.ArgumentParser()

    parser.add_argument('--input_news_file_path',
                       default='../../../label/political_news_label_0_to_99.csv',
                       help='enter input news data csv file path')

    parser.add_argument('--output_all_output_file_path',
                       default='./result/v0_Total_Opinion.csv',
                       help='enter output_all_output_file_path')

    parser.add_argument('--output_opinion_after_cluster_file_path',
                       default='./result/v0_cluster_opinions.csv',
                       help='enter output_opinion_after_cluster_file_path')

    parser.add_argument('--output_topic_info_df_file_path',
                       default='./result/v0_topic_info.csv',
                       help='enter output_topic_info_df_file_path')
    
    args = parser.parse_args()
    file_full_name = args.input_news_file_path

    # call top function
    news_df,new_cluster_opinions_df,topic_info_df = top(file_full_name)

    # change the dataframe to csv
    all_output_file = args.output_all_output_file_path
    opinion_after_cluster_file = args.output_opinion_after_cluster_file_path
    topic_info_file = args.output_topic_info_df_file_path

    # os.makedirs(os.path.dirname(path), exist_ok=True)
    pathlib.Path(all_output_file).parent.mkdir(parents=True, exist_ok=True)
    pathlib.Path(opinion_after_cluster_file).parent.mkdir(parents=True, exist_ok=True)
    pathlib.Path(topic_info_file).parent.mkdir(parents=True, exist_ok=True)

    news_df.to_csv(all_output_file, index=False)
    new_cluster_opinions_df.to_csv(opinion_after_cluster_file,index=False)
    topic_info_df.to_csv(topic_info_file,index=False)